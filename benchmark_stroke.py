import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import lightgbm as lgb
from sklearn.ensemble import RandomForestClassifier, IsolationForest
from sklearn.linear_model import LogisticRegression
from sklearn.svm import OneClassSVM
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, roc_auc_score, accuracy_score
from sklearn.preprocessing import StandardScaler
from data_loader import load_stroke_data # ä¿æŒä½ çš„æ•°æ®åŠ è½½

# ==========================================
# 1. è®¾ç½®æµ·æŠ¥çº§ç»˜å›¾é£æ ¼ (åŠ å¤§åŠ ç²—)
# ==========================================
plt.rcParams.update({
    'font.family': 'sans-serif',
    'font.size': 16,
    'axes.labelsize': 20,
    'axes.labelweight': 'bold',
    'xtick.labelsize': 16,
    'ytick.labelsize': 16,
    'axes.linewidth': 2,
    'legend.fontsize': 16,
    'figure.figsize': (14, 8)
})

# ==========================================
# 2. æ•°æ®å‡†å¤‡
# ==========================================
print("æ­£åœ¨åŠ è½½æ•°æ®...")
# åŠ è½½åŸå§‹æ•°æ®
X_raw, y = load_stroke_data("healthcare-dataset-stroke-data.csv")

# ã€é‡è¦é¢„å¤„ç†ã€‘ä¸ºäº†è®©æ‰€æœ‰æ¨¡å‹(åŒ…æ‹¬RFå’ŒLR)éƒ½èƒ½è·‘ï¼Œæˆ‘ä»¬éœ€è¦æŠŠåˆ†ç±»å˜é‡è½¬ä¸ºæ•°å­—
# LightGBMå…¶å®ä¸éœ€è¦è¿™ä¸€æ­¥ï¼Œä½†ä¸ºäº†å…¬å¹³å¯¹æ¯”å’Œä»£ç ä¸æŠ¥é”™ï¼Œæˆ‘ä»¬ç»Ÿä¸€åš One-Hot ç¼–ç 
print("æ­£åœ¨è¿›è¡Œç‰¹å¾ç¼–ç  (One-Hot Encoding)...")
X_encoded = pd.get_dummies(X_raw, drop_first=True)

# æŸäº›æ¨¡å‹(å¦‚LR)å¯¹æ•°å€¼å¹…åº¦æ•æ„Ÿï¼Œå»ºè®®æ ‡å‡†åŒ– (è™½ç„¶æ ‘æ¨¡å‹ä¸éœ€è¦ï¼Œä½†åŠ äº†ä¹Ÿæ²¡åå¤„)
scaler = StandardScaler()
X_scaled = pd.DataFrame(scaler.fit_transform(X_encoded), columns=X_encoded.columns)

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# ==========================================
# 3. å®šä¹‰å››å¤§é‡‘åˆšæ¨¡å‹
# ==========================================
# æ³¨æ„ï¼šæ‰€æœ‰æœ‰ç›‘ç£æ¨¡å‹éƒ½å¼€å¯ class_weight='balanced' ä»¥åº”å¯¹ä¸å¹³è¡¡
models = {
    # --- ä½ çš„ä¸»è§’ ---
    # ä¿®æ”¹ LightGBM æ¨¡å‹å®šä¹‰ï¼Œé‡‡ç”¨â€œå°æ­¥æ…¢è·‘ + é™åˆ¶å¤æ‚åº¦â€ç­–ç•¥
'LightGBM (Tuned)': lgb.LGBMClassifier(
    random_state=42, 
    class_weight='balanced', 
    verbose=-1,
    
    # --- å…³é”®è°ƒå‚åŒºåŸŸ ---
    n_estimators=500,     # å¢åŠ æ ‘çš„æ•°é‡ (åŸæœ¬100)
    learning_rate=0.02,   # é™ä½å­¦ä¹ ç‡ï¼Œå­¦å¾—æ›´ç»† (åŸæœ¬0.1)
    
    num_leaves=15,        # å‡å°‘å¶å­ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ (åŸæœ¬31)
    max_depth=4,          # é™åˆ¶æ·±åº¦ï¼Œåªçœ‹ä¸»è¦ç‰¹å¾ (åŸæœ¬æ— é™åˆ¶)
    
    min_child_samples=30, # æ¯ä¸ªå¶å­è‡³å°‘è¦åŒ…å«30ä¸ªæ ·æœ¬ï¼Œé¿å…é’ˆå¯¹ä¸ªä¾‹
    reg_alpha=0.1,        # L1 æ­£åˆ™åŒ– (ç¨å¾®æƒ©ç½šä¸€ä¸‹å¤æ‚çš„æƒé‡)
    reg_lambda=0.1        # L2 æ­£åˆ™åŒ–
),
    
    # --- å¼ºåŠ›æœ‰ç›‘ç£åŸºçº¿ ---
    'Random Forest': RandomForestClassifier(random_state=42, class_weight='balanced', n_jobs=-1),
    
    # --- ç®€å•çº¿æ€§åŸºçº¿ ---
    'Logistic Reg.': LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000),
    
    # --- æ— ç›‘ç£åŸºçº¿ (æµ·æŠ¥åŸæœ¬çš„å¯¹æ¯”) ---
    # å­¤ç«‹æ£®æ—æ˜¯æ— ç›‘ç£çš„ï¼Œä¸èƒ½ç”¨ class_weightï¼Œä¹Ÿæ²¡æœ‰ fit(X, y) åªæœ‰ fit(X)
    'Isolation Forest': 'Unsupervised_IF' 
}

# ==========================================
# 4. è®­ç»ƒä¸è¯„ä¼°
# ==========================================
results = {'Model': [], 'AUC': [], 'F1-Score': []}

print("-" * 50)
for name, model in models.items():
    print(f"æ­£åœ¨è·‘æ¨¡å‹: {name}...")
    
    if name == 'Isolation Forest':
        # å­¤ç«‹æ£®æ—ç‰¹æ®Šå¤„ç† (æ— ç›‘ç£)
        clf = IsolationForest(contamination=0.05, random_state=42, n_jobs=-1)
        clf.fit(X_train) # ä¸ç»™ y
        
        # é¢„æµ‹ï¼š-1æ˜¯å¼‚å¸¸ï¼Œ1æ˜¯æ­£å¸¸ã€‚æˆ‘ä»¬éœ€è¦è½¬æ¢æˆ 0(æ­£å¸¸) å’Œ 1(å¼‚å¸¸)
        y_pred_raw = clf.predict(X_test)
        y_pred = np.where(y_pred_raw == -1, 1, 0)
        # å­¤ç«‹æ£®æ—æ²¡æœ‰æ ‡å‡†çš„ predict_probaï¼Œæˆ‘ä»¬ç”¨ decision_function è¿‘ä¼¼
        y_score = -clf.decision_function(X_test) # è¶Šå°è¶Šå¼‚å¸¸ï¼Œæ‰€ä»¥å–è´Ÿ
        
    else:
        # æœ‰ç›‘ç£æ¨¡å‹æ ‡å‡†æµç¨‹
        model.fit(X_train, y_train)
        
        # è·å–å±äºç±»åˆ« 1 (ä¸­é£) çš„æ¦‚ç‡
        y_score = model.predict_proba(X_test)[:, 1]
        
        # ğŸŸ¢ã€æ ¸å¿ƒä¿®æ”¹ã€‘ä¸è¦ç›´æ¥ç”¨ predict()ï¼Œé‚£æ˜¯åŸºäº 0.5 é˜ˆå€¼çš„
        # å¯¹äºä¸å¹³è¡¡æ•°æ®ï¼Œæˆ‘ä»¬æŠŠé˜ˆå€¼é™åˆ° 0.2 æˆ– 0.15 (æ ¹æ®ä½ çš„å®é™…æƒ…å†µå¾®è°ƒ)
        # æ„æ€å°±æ˜¯ï¼šåªè¦ä¸­é£æ¦‚ç‡è¶…è¿‡ 20%ï¼Œå°±åˆ¤å®šä¸ºä¸­é£
        threshold = 0.15  # ä½ å¯ä»¥è¯• 0.15, 0.2, 0.25
        y_pred = (y_score > threshold).astype(int)

    # è®¡ç®—æŒ‡æ ‡ (ä»£ç ä¸å˜)
    auc = roc_auc_score(y_test, y_score)
    f1 = f1_score(y_test, y_pred)
    # è®¡ç®—æŒ‡æ ‡
    auc = roc_auc_score(y_test, y_score)
    f1 = f1_score(y_test, y_pred)
    
    results['Model'].append(name)
    results['AUC'].append(auc)
    results['F1-Score'].append(f1)
    
    print(f"   >> AUC: {auc:.4f} | F1: {f1:.4f}")

# ==========================================
# 5. ç»˜åˆ¶æµ·æŠ¥çº§å¯¹æ¯”å›¾ (Bar Chart)
# ==========================================
df_res = pd.DataFrame(results)

# è®¾ç½®æŸ±çŠ¶å›¾ä½ç½®
x = np.arange(len(df_res['Model']))
width = 0.35  # æŸ±å­å®½åº¦

fig, ax = plt.subplots()

# ç”»ä¸¤ç»„æŸ±å­
rects1 = ax.bar(x - width/2, df_res['AUC'], width, label='AUC', color='#2ca02c') # ç»¿è‰²
rects2 = ax.bar(x + width/2, df_res['F1-Score'], width, label='F1-Score', color='#1f77b4') # è“è‰²

# è®¾ç½®æ ‡ç­¾å’Œæ ‡é¢˜
ax.set_ylabel('Score', fontweight='bold')
ax.set_title('Comprehensive Model Comparison', fontweight='bold', pad=20)
ax.set_xticks(x)
ax.set_xticklabels(df_res['Model'], fontweight='bold')
ax.set_ylim(0, 1.1) # Yè½´ç¨å¾®ç•™ç‚¹ç©º
ax.legend(loc='upper left', frameon=True, fancybox=True, shadow=True) # å›¾ä¾‹åŠ ä¸ªæ¡†æ›´æ¸…æ¥š

# ç»™æŸ±å­ä¸Šæ–¹æ ‡æ•°å€¼ (è®©è¯„å§”ä¸€çœ¼çœ‹åˆ°æ•°æ®)
def autolabel(rects):
    for rect in rects:
        height = rect.get_height()
        ax.annotate(f'{height:.2f}',
                    xy=(rect.get_x() + rect.get_width() / 2, height),
                    xytext=(0, 3),  # 3 points vertical offset
                    textcoords="offset points",
                    ha='center', va='bottom', fontsize=14, fontweight='bold')

autolabel(rects1)
autolabel(rects2)

# åŠ ä¸€æ¡æ°´å¹³çº¿æ ‡å‡º Baseline (æ¯”å¦‚ 0.5)
ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)
ax.text(3.6, 0.51, 'Random Guess', fontsize=12, color='gray')

plt.tight_layout()
plt.savefig("poster_model_comparison_v2.png", dpi=300)
plt.show()

print("-" * 50)
print("âœ… æ–°çš„å¯¹æ¯”å›¾å·²ä¿å­˜ä¸º poster_model_comparison_v2.png")
print("å¿«å»çœ‹çœ‹ LightGBM æ˜¯ä¸æ˜¯é¥é¥é¢†å…ˆï¼")