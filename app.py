import streamlit as st
import pandas as pd
import joblib
import os
import shap
import numpy as np

# å¯¼å…¥é…ç½®å’Œæ ¸å¿ƒæ¨¡å—
from config import PATHS, FILES
from src.repair_module import AnomalyRepairer
from src.utils import process_and_train, save_system_state, load_system_state

# ==========================================
# 1. é¡µé¢é…ç½®ä¸çŠ¶æ€åˆå§‹åŒ–
# ==========================================
st.set_page_config(page_title="Mixed-Type Anomaly Detection System", layout="wide")

# --- ã€å…³é”®ä¿®æ”¹ã€‘åˆå§‹åŒ– Session State (è®°å¿†æ¨¡å—) ---
# å¦‚æœç³»ç»Ÿç¬¬ä¸€æ¬¡å¯åŠ¨ï¼Œå…ˆåœ¨å†…å­˜é‡Œå»ºå‡ ä¸ª"ç©ºæŠ½å±‰"æ¥æ”¾æ•°æ®
if 'uploaded_df' not in st.session_state:
    st.session_state.uploaded_df = None  # å­˜æ”¾ä¸Šä¼ çš„æ•°æ®
if 'train_metrics' not in st.session_state:
    st.session_state.train_metrics = None # å­˜æ”¾è®­ç»ƒåˆ†æ•°
if 'is_trained' not in st.session_state:
    st.session_state.is_trained = False   # è®°å½•æ˜¯å¦è®­ç»ƒè¿‡

# Sidebar å¯¼èˆª
st.sidebar.title("ğŸ“Œ Navigation")
page = st.sidebar.radio("Go to", ["1. Data & Model Training", "2. Detection & Repair"])

# =========================================================
# é¡µé¢ 1: æ•°æ®ä¸Šä¼ ä¸è®­ç»ƒ
# =========================================================
if page == "1. Data & Model Training":
    st.title("ğŸ› ï¸ System Setup: Data Import & Training")
    st.markdown("Upload your mixed-type dataset (CSV) to build the anomaly detection model.")
    
    # 1. æ–‡ä»¶ä¸Šä¼ 
    # æ³¨æ„ï¼šåˆ‡æ¢é¡µé¢å file_uploader æ§ä»¶æœ¬èº«ä¼šé‡ç½®ï¼Œè¿™æ˜¯ Streamlit çš„ç‰¹æ€§
    # ä½†æˆ‘ä»¬æŠŠè¯»å–åçš„æ•°æ®å­˜åˆ°äº† session_state é‡Œï¼Œæ‰€ä»¥æ•°æ®ä¸ä¼šä¸¢
    uploaded_file = st.file_uploader("Upload CSV File", type=["csv"])
    
    # å¦‚æœç”¨æˆ·åˆšä¸Šä¼ äº†æ–°æ–‡ä»¶
    if uploaded_file is not None:
        try:
            df = pd.read_csv(uploaded_file)
            st.session_state.uploaded_df = df  # ã€å­˜å…¥è®°å¿†ã€‘
            # å¦‚æœä¸Šä¼ äº†æ–°æ–‡ä»¶ï¼Œé‡ç½®è®­ç»ƒçŠ¶æ€
            st.session_state.is_trained = False 
            st.session_state.train_metrics = None
        except Exception as e:
            st.error(f"Error reading file: {e}")

    # 2. æ£€æŸ¥è®°å¿†ä¸­æ˜¯å¦æœ‰æ•°æ®
    if st.session_state.uploaded_df is not None:
        df = st.session_state.uploaded_df
        st.success(f"Dataset Loaded: {df.shape[0]} rows, {df.shape[1]} columns")
        st.dataframe(df.head())
        
        # =========================================================
        # ğŸ“Š æ•°æ®ç»Ÿè®¡é¢æ¿
        # =========================================================
        with st.expander("ğŸ“Š Data Statistics & Quality Report", expanded=False):
            # --- åŸºç¡€ä¿¡æ¯ ---
            st.markdown("#### ğŸ“‹ Basic Information")
            col1, col2, col3, col4 = st.columns(4)
            col1.metric("Total Rows", f"{df.shape[0]:,}")
            col2.metric("Total Columns", df.shape[1])
            col3.metric("Missing Values", f"{df.isnull().sum().sum():,}")
            col4.metric("Memory Usage", f"{df.memory_usage(deep=True).sum() / 1024:.1f} KB")
            
            st.markdown("---")
            
            # --- æ•°æ®ç±»å‹åˆ†å¸ƒ ---
            st.markdown("#### ğŸ·ï¸ Data Types Distribution")
            dtype_counts = df.dtypes.astype(str).value_counts()
            col_type1, col_type2 = st.columns(2)
            
            with col_type1:
                # æ•°å€¼å‹åˆ—
                numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
                st.markdown(f"**Numeric Columns** ({len(numeric_cols)})")
                if numeric_cols:
                    st.write(", ".join(numeric_cols))
                else:
                    st.write("None")
            
            with col_type2:
                # åˆ†ç±»å‹åˆ—
                cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
                st.markdown(f"**Categorical Columns** ({len(cat_cols)})")
                if cat_cols:
                    st.write(", ".join(cat_cols))
                else:
                    st.write("None")
            
            st.markdown("---")
            
            # --- ç¼ºå¤±å€¼åˆ†æ ---
            st.markdown("#### â“ Missing Values Analysis")
            missing_data = df.isnull().sum()
            missing_data = missing_data[missing_data > 0].sort_values(ascending=False)
            
            if len(missing_data) > 0:
                missing_df = pd.DataFrame({
                    'Column': missing_data.index,
                    'Missing Count': missing_data.values,
                    'Missing %': (missing_data.values / len(df) * 100).round(2)
                })
                st.dataframe(missing_df, use_container_width=True, hide_index=True)
            else:
                st.success("âœ… No missing values found!")
            
            st.markdown("---")
            
            # --- æ•°å€¼ç‰¹å¾ç»Ÿè®¡ ---
            if numeric_cols:
                st.markdown("#### ğŸ“ˆ Numeric Features Statistics")
                st.dataframe(df[numeric_cols].describe().T.round(2), use_container_width=True)
            
            # --- åˆ†ç±»ç‰¹å¾åˆ†å¸ƒ ---
            if cat_cols:
                st.markdown("#### ğŸ“Š Categorical Features Distribution")
                selected_cat = st.selectbox("Select a categorical column to view distribution:", cat_cols)
                if selected_cat:
                    value_counts = df[selected_cat].value_counts()
                    dist_df = pd.DataFrame({
                        'Value': value_counts.index,
                        'Count': value_counts.values,
                        'Percentage': (value_counts.values / len(df) * 100).round(2)
                    })
                    st.dataframe(dist_df, use_container_width=True, hide_index=True)
        
        # é€‰æ‹©ç›®æ ‡åˆ—
        target_col = st.selectbox("Select the Target Column (Label)", df.columns, index=len(df.columns)-1)
        
        # --- ç›®æ ‡åˆ—åˆ†å¸ƒé¢„è§ˆ ---
        if target_col:
            target_counts = df[target_col].value_counts()
            col_t1, col_t2, col_t3 = st.columns(3)
            
            total = len(df)
            normal_count = target_counts.get(0, 0)
            anomaly_count = target_counts.get(1, 0)
            
            col_t1.metric("Normal (0)", f"{normal_count:,}", f"{normal_count/total*100:.1f}%")
            col_t2.metric("Anomaly (1)", f"{anomaly_count:,}", f"{anomaly_count/total*100:.1f}%")
            col_t3.metric("Imbalance Ratio", f"1:{normal_count//max(anomaly_count,1)}" if anomaly_count > 0 else "N/A")
            
            st.info(f"The system will learn to detect anomalies based on '{target_col}'. (0=Normal, 1=Anomaly)")
        
        # 3. è®­ç»ƒæŒ‰é’®
        if st.button("ğŸš€ Start Training Model"):
            with st.spinner('Training LightGBM model and preparing repair database...'):
                # è°ƒç”¨ utils
                model, X_test, normal_data, metrics, feats = process_and_train(df, target_col)
                
                # ä¿å­˜åˆ°ç¡¬ç›˜ï¼ˆä½¿ç”¨é…ç½®ä¸­çš„è·¯å¾„ï¼‰
                save_system_state(model, X_test, normal_data, feats)
                
                # ã€å­˜å…¥è®°å¿†ã€‘
                st.session_state.train_metrics = metrics
                st.session_state.is_trained = True
                
            st.success("âœ… Training Complete!")
            st.balloons()

    # 4. æ˜¾ç¤ºè®­ç»ƒç»“æœ (å³ä½¿åˆ·æ–°é¡µé¢ï¼Œåªè¦ session_state é‡Œæœ‰ï¼Œå°±æ˜¾ç¤º)
    if st.session_state.is_trained and st.session_state.train_metrics is not None:
        st.markdown("---")
        st.subheader("ğŸ“Š Model Performance")
        col_m1, col_m2 = st.columns(2)
        col_m1.metric("F1-Score", f"{st.session_state.train_metrics['f1']:.4f}")
        col_m2.metric("AUC-ROC", f"{st.session_state.train_metrics['auc']:.4f}")
        
        st.markdown("ğŸ‘‰ **Now go to '2. Detection & Repair' page to test the system.**")

# =========================================================
# é¡µé¢ 2: æ£€æµ‹ä¸ä¿®å¤
# =========================================================
elif page == "2. Detection & Repair":
    st.title("ğŸ” Interactive Detection & Repair")
    
    # æ£€æŸ¥ç¡¬ç›˜ä¸Šæœ‰æ²¡æœ‰æ¨¡å‹æ–‡ä»¶ (è¿™æ˜¯ä¸ºäº†é˜²æ­¢ç”¨æˆ·ç›´æ¥è·³åˆ°è¿™ä¸€é¡µ)
    if not os.path.exists(FILES["model"]):
        st.warning("âš ï¸ No model found. Please go to 'Data & Model Training' page first.")
        st.stop()
        
    # åŠ è½½æ¨¡å‹ (ä½¿ç”¨ cache_resource é¿å…é‡å¤åŠ è½½)
    @st.cache_resource
    def load_model_resources():
        return load_system_state()

    model, X_test, normal_data = load_model_resources()
    
    # âš¡ SHAP Explainer ç¼“å­˜ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
    # TreeExplainer åˆ›å»ºå¼€é”€å¤§ï¼Œç¼“å­˜ååªåˆ›å»ºä¸€æ¬¡
    @st.cache_resource
    def get_shap_explainer(_model):
        """ç¼“å­˜ SHAP explainerï¼Œé¿å…é‡å¤åˆ›å»º"""
        return shap.TreeExplainer(_model)
    
    explainer = get_shap_explainer(model)
    
    # åˆå§‹åŒ–ä¿®å¤å™¨
    if 'repairer' not in st.session_state:
        st.session_state.repairer = AnomalyRepairer(normal_data)
        
    # é˜²æ­¢ç´¢å¼•æŠ¥é”™
    max_len = len(X_test) - 1
    if max_len < 0: max_len = 0
    
    # =========================================================
    # ä½¿ç”¨ Tabs åŒºåˆ†å•æ¡æ£€æµ‹å’Œæ‰¹é‡æ£€æµ‹
    # =========================================================
    tab1, tab2 = st.tabs(["ğŸ”¬ Single Detection", "ğŸ“Š Batch Detection & Export"])
    
    # ---------------------------------------------------------
    # Tab 1: å•æ¡æ£€æµ‹
    # ---------------------------------------------------------
    with tab1:
        st.sidebar.markdown("---")
        st.sidebar.header("ğŸ”¬ Single Detection")
        
        sample_id = st.sidebar.number_input(
            "Enter Test Sample ID", 
            min_value=0, 
            max_value=max_len, 
            value=0, 
            step=1,
            help=f"Valid range: 0 to {max_len}"
        )
        
        try:
            sample_data = X_test.iloc[[sample_id]]
            
            c1, c2 = st.columns([1, 2])
            with c1:
                st.subheader("Target Profile")
                st.dataframe(sample_data.T, height=400)
                
            with c2:
                st.subheader("Analysis Result")
                if st.button("Run Diagnosis", key="run_diag"):
                    pred = model.predict(sample_data)[0]
                    prob = model.predict_proba(sample_data)[0][1]
                    
                    if pred == 1:
                        st.error(f"ğŸš¨ ANOMALY DETECTED (Risk Score: {prob:.4f})")
                        
                        # SHAP è§£é‡Šï¼ˆä½¿ç”¨ç¼“å­˜çš„ explainerï¼‰
                        shap_values = explainer.shap_values(sample_data)
                        vals = shap_values[1][0] if isinstance(shap_values, list) else shap_values[0]
                        
                        top_indices = np.argsort(vals)[::-1][:3]
                        feat_names = sample_data.columns
                        
                        st.markdown("### ğŸ› ï¸ Smart Repair Suggestions")
                        
                        for idx in top_indices:
                            if vals[idx] > 0:
                                fname = feat_names[idx]
                                report, _ = st.session_state.repairer.generate_repair_suggestion(sample_data, fname)
                                
                                with st.expander(f"ğŸ”´ Issue: {fname} (Impact: +{vals[idx]:.2f})", expanded=True):
                                    st.write(f"**Current:** {sample_data[fname].values[0]}")
                                    st.success(f"**Suggested:** {report['Suggested Value']}")
                                    st.caption(f"Reasoning: {report['Repair Logic']}")
                    else:
                        st.success(f"âœ… Normal Profile (Risk Score: {prob:.4f})")
        except Exception as e:
            st.error(f"Error analyzing sample: {e}")
    
    # ---------------------------------------------------------
    # Tab 2: æ‰¹é‡æ£€æµ‹ + å¯¼å‡º
    # ---------------------------------------------------------
    with tab2:
        st.markdown("### ğŸ“Š Batch Anomaly Detection")
        st.markdown("Scan multiple samples at once and export results to CSV.")
        
        # é€‰æ‹©æ£€æµ‹èŒƒå›´
        col_range1, col_range2 = st.columns(2)
        with col_range1:
            detection_mode = st.radio(
                "Detection Scope",
                ["All Test Samples", "Custom Range"],
                horizontal=True
            )
        
        if detection_mode == "Custom Range":
            with col_range2:
                range_start = st.number_input("Start Index", min_value=0, max_value=max_len, value=0)
                range_end = st.number_input("End Index", min_value=0, max_value=max_len, value=min(100, max_len))
        else:
            range_start, range_end = 0, max_len
        
        # æ‰¹é‡æ£€æµ‹æŒ‰é’®
        if st.button("ğŸš€ Run Batch Detection", key="batch_detect", type="primary"):
            with st.spinner(f"Scanning samples {range_start} to {range_end}..."):
                # è·å–æŒ‡å®šèŒƒå›´çš„æ•°æ®
                batch_data = X_test.iloc[range_start:range_end+1]
                
                # æ‰¹é‡é¢„æµ‹
                predictions = model.predict(batch_data)
                probabilities = model.predict_proba(batch_data)[:, 1]
                
                # æ„å»ºç»“æœ DataFrame
                results_df = batch_data.copy()
                results_df.insert(0, 'Sample_ID', range(range_start, range_end+1))
                results_df['Prediction'] = predictions
                results_df['Risk_Score'] = probabilities.round(4)
                results_df['Status'] = np.where(predictions == 1, 'ğŸš¨ Anomaly', 'âœ… Normal')
                
                # ä¿å­˜åˆ° session_state
                st.session_state.batch_results = results_df
                st.session_state.batch_stats = {
                    'total': len(results_df),
                    'anomalies': int((predictions == 1).sum()),
                    'normals': int((predictions == 0).sum())
                }
            
            st.success("âœ… Batch detection complete!")
        
        # æ˜¾ç¤ºç»“æœ
        if 'batch_results' in st.session_state and st.session_state.batch_results is not None:
            stats = st.session_state.batch_stats
            results_df = st.session_state.batch_results
            
            # ç»Ÿè®¡æŒ‡æ ‡
            st.markdown("---")
            st.markdown("### ğŸ“ˆ Detection Summary")
            col_s1, col_s2, col_s3, col_s4 = st.columns(4)
            col_s1.metric("Total Scanned", f"{stats['total']:,}")
            col_s2.metric("Anomalies Found", f"{stats['anomalies']:,}", 
                         delta=f"{stats['anomalies']/stats['total']*100:.1f}%", delta_color="inverse")
            col_s3.metric("Normal Samples", f"{stats['normals']:,}")
            col_s4.metric("Anomaly Rate", f"{stats['anomalies']/stats['total']*100:.2f}%")
            
            st.markdown("---")
            
            # ç­›é€‰é€‰é¡¹
            filter_option = st.radio(
                "Filter Results",
                ["All", "Anomalies Only", "Normal Only"],
                horizontal=True
            )
            
            if filter_option == "Anomalies Only":
                display_df = results_df[results_df['Prediction'] == 1]
            elif filter_option == "Normal Only":
                display_df = results_df[results_df['Prediction'] == 0]
            else:
                display_df = results_df
            
            # æ˜¾ç¤ºç»“æœè¡¨æ ¼
            st.markdown(f"### ğŸ“‹ Results ({len(display_df)} samples)")
            st.dataframe(
                display_df[['Sample_ID', 'Status', 'Risk_Score'] + list(X_test.columns)],
                use_container_width=True,
                height=400
            )
            
            st.markdown("---")
            
            # å¯¼å‡ºåŠŸèƒ½
            st.markdown("### ğŸ“¥ Export Results")
            col_exp1, col_exp2 = st.columns(2)
            
            with col_exp1:
                # å¯¼å‡ºå…¨éƒ¨ç»“æœ
                csv_all = results_df.to_csv(index=False).encode('utf-8')
                st.download_button(
                    label="ğŸ“¥ Download All Results (CSV)",
                    data=csv_all,
                    file_name="batch_detection_all.csv",
                    mime="text/csv",
                    key="download_all"
                )
            
            with col_exp2:
                # åªå¯¼å‡ºå¼‚å¸¸
                anomalies_df = results_df[results_df['Prediction'] == 1]
                if len(anomalies_df) > 0:
                    csv_anomalies = anomalies_df.to_csv(index=False).encode('utf-8')
                    st.download_button(
                        label="ğŸš¨ Download Anomalies Only (CSV)",
                        data=csv_anomalies,
                        file_name="batch_detection_anomalies.csv",
                        mime="text/csv",
                        key="download_anomalies"
                    )
                else:
                    st.info("No anomalies found to export.")